{
    "ukhsa": {
        "headline": "Fighting COVID with UKHSA",
        "intro": "Working as part of the UK Health Security Agency as part of the response to the coronavirus pandemic, I performed in a data specialist role encompassing analysis, data science, engineering and devops.",
        "technologies": ["AWS CDK", "AWS Glue", "AWS Lambda", "Python", "Jupyter"],
        "explanation": [
            "In November 2020, I joined the United Kingdom Health Security Agency (UKHSA), then known as the Joint Biosecurity Centre, as a contractor with the purpose to try and leverage data and data science to fight the coronavirus pandemic that was ravaging the country. I was assigned to a new team focused on using waste water (sewage) data to detect outbreaks and measure cases in an area. This was a cutting edge implementation of wastewater based epidemiology at a scale of which had never been seen before anywhere in the world.",
            "The first project I worked on was to, produce bespoke visualisations for this new data source to convey simple meanings of complex subject matter. We were measuring the amount of covid molecules in the waste water samples which is not a simply understood metric and so it was imperative we made concise and digestible figures with which to display these results. We would then produce reports with our figures which would go to descision makers such as public health officials who would use them as part of their descision making process to shape public health policy. At this stage it was still a very manual process and so this was taking my team and I most of our time each week with the constant stream of data we were receiving each day, as such I began the process of automation.",
            "In the initial automating of this I produced some software to run our figure producing code and create slide decks in a fixed format with new latest figures added. This step of automation was huge for the team as it freed up the majority of everyone else's working time as I could now simply do the job which previously 3 of us were doing all week. This allowed the team to rapidly address other areas of improvement with all the new available time. However, I carried on with the automation work as there were still outstanding issues with it which I wanted to address and so I then, unprompted and single handedly, completely rewrote all of the automation code and figure producing code and created a new report which was fully interactive and functioned much like a dashboard. This allowed for a far deeper level of investigation and insights into our data as well as showing more data as there was no risk of static figures becoming too cluttered, this work was hugely significant and I won an award for my contributions to the civil service.",
            "Another large area of my contributions was on the data engineering side of the project, I did a lot of work designing and building our DE solution. Obviously, we wanted as much data about the pandemic as possible so that we could produce the best possible insights and this meant I was regularly building more and more sophisticated datasets. This work was to abstract any cleaning or processing of the data away from our data scientists and allow them to very simply access reliable and ready to use datasets. Again this removed a lot of unnecessary data wrangling work and saved the organisation many hours of man time which could be better focussed on other problems.",
            "Our data engineering solution was fully cloud based, I was also instrumental in making sure our entire solution was deployed using infrastructure as code. Utilising AWS cloud development kit we ensured the entire solution could rapidly be redeployed and fully functional simply by deploying our code.",
            "I also ran some educational sessions during my time at UKHSA, these focussed on teaching about Git and general cloud practices to the wider team as many of them had no experience with these technologies."
        ],
        "outcomes": [
            "Improved health outcomes and provably saved lives by early and accurate detections", 
            "Automation of reporting processes allowed for redistribution of work and generated tens of thousands pounds in efficiency savings", 
            "Fully transformed the data engineering solution to infrastructure as code allowing for rapid deployment and scale ups",
            "Created a ML and rules based system utilising a proprietary algorithm for determining high risk areas and assigning a risk score",
            "Development of sophisticated visualisations allowed for easier insight gathering from our dataset and improved information for policy makers",
            "Built end-to-end data pipelines to ensure high quality, validated data sources in simple outputs for scientific analysis"
        ]
    },
    "insuranceai": {
        "headline": "AI health insurance",
        "intro": "Built a sophisticated reasoning engine and utilised natural language processing to automate the role of underwriters to calculate health insurance policy pricing based on a customers medical history.",
        "technologies": ["Python", "IBM Watson", "NLP", "AWS"],
        "explanation": [
            "This project was a cutting edge use of natural language processing and cognitive reasoning, at the time it made Aviva the only insurance company in the UK to have this capability. It focussed on using a customer's medical history data (which they already provided) when they applied for health and life insurance products and passing it through our pipeline to remove any need for human underwriter involvement.",
            "The aim was that by automating this process it would allow for a much faster throughput of cases as previously it could take as long as 6 weeks for a customer to recieve a quote from when they submit their initial request. Obviously, this was terrible for both the customer and the business and so this solution aimed to fix that. Furthermore, by handling the majority of the cases it would allow the underwriters just to focus their expertise and the few cases that were particularly complex or where the system had low confidence in its predictions. All in all these time and efficiency savings would be able to save the company over £10M in just a few years.",
            "There were many large challenges to overcome whilst building the solution, not least of which being whether it was even possible. As this solution was world leading there was no guides we could follow or articles to read we had to break new ground. Additionally, utilising NLP heavily came with its own set of problems particularly when the medical records were often verbatim dictacted text or even shorthand.",
            "Our solution was built mainly using two components, firstly a sophisticated NLP engine (IBM Watson), and secondly an entirely in-house built reasoning engine. The process would be that when a customer applies online, their medical history is downloaded and passed into the NLP engine from which many conditions, symptoms and other relevant health information would be extracted. Additionally, it would also extract the links between these conditions and a certainty in its estimates of the links. All of this infomation would then be passed into our reasoning engine which would take the statements extracted from the medical history and use them to build up a 'health profile' estimating the likelihood of many diseases and conditions the individual could have. It would also include other non-conditions but relevant information such as family history, height, weight, age, medications, etc and it would also adjust this by time as something a long time ago is obviously less relevant for pricing now. Once it had this it was able to calculate the expected price for insurance products based on the health of the patient.",
            "The project was an unmitigated success with it successfully managing to price over 70% of the cases fed into it. This allowed all of the predicted business savings to be achieved as well as significantly decreasing the time taken for customers to receive their quotes. As far as I am aware the system is still live and running in production."
        ],
        "outcomes": [
            "Successful automation of simple cases allowed for reallocation of human resource to complex cases only", 
            "Fully automated pipeline decreased wait time for insurance quote from 6 weeks to minutes",
            "Speed up and resource savings resulted in business savings in excess of £10M",
            "Form based responses were also able to be priced and fully automated resulting in further business savings of ~£1M"
        ]
    },
    "softwarevalidation": {
        "headline": "Highly Secure Software Validation",
        "intro": "Created a platform to validate and inspect all ingested software for any malicious behaviours before storing in an assured database for secure access.",
        "technologies": ["Javascript", "Ansible", "VMWare", "Apache NiFi", "Elasticsearch"],
        "explanation": [
            "Whilst working with BT, I assisted in a multi-billion pound project supplying a highly secure solution. Due to the highly secretive nature of the work I am not at liberty to discuss much of what was done but in this breakdown I will provide as much information as I can.",
            "My contribution in the project was centered around the entry point for software to the platform. As the security of the system was of the upmost importance we were building an entry point where a user could upload files, these files would then be analysed. Once a piece of uploaded software had gone through this analysis it would have either passed or failed and then could move it through to be available in the secure area.",
            "Over the duration of this project I faced some huge hurdles and challenges, particularly due to the focus on security. Firstly, our code needed to be secure and tested to highly rigourous standards. Additionally, the system was going to be installed on premisis and as such meant it was entirely air-gapped, therefore we were unable to properly run and test our code on the hardware without significant difficulty. This was significant as there was a lot of computation work being down in the software evaluations.",
            "There were many aspects to the solution, it needed to be able to move potentially very large files through without issue. It needed to have state of the art cyber security capabilities to properly break down and inspect the files passing through. It needed to have a database to store both the files themselves and significant metadata and results about them. Finally, it needed a bespoke web application through which these files could be accessed, viewed and downloaded from. As one of the lead developers on the project I had a hand in development for every one of these aspects and requirements.",
            "The movement of files through the system was handled using Apache NiFi, we were also able to implement custom processors to help it interact more nicely with the other components in the system. This was the backbone of the solution the flow was essentially NiFi taking files, passing to components which write them to another location from which NiFi would pick them up and proceed them through the process. For the cyber security solutions we implemented multiple systems both internally developed and integrated external products. Our intention was that we would cover absolutely every aspect of software investigation to give the highest possible confidence in the security of the files. Finally, the processed files would be stored and their metadata stored in an Elasticsearch database, using this data I could then develop a frontend to display the files along with their metadata and make them available. This also came with search, filter and sorting on a wide range of characteristics.",
            "The project was very successful in my time on it. We were able to build the solution both on time and within budget, this was very important as we were a key dependency on other areas further of the wider solution and delays from us would cause cascading delays down the line. Another huge success was the full automation of the deployment, this meant the solution was fully redeployable in the event of outage but was also done modularly such that any individual VM or service going down would also be able to be completely rebuilt quickly and without hassle. Furthermore, we were able to repurpose the previous solution into a test environment so that going forwards development would be far easier."
        ],
        "outcomes": [
            "Successfully built system on time and within budget, preventing costly delays in a £1.5bn project", 
            "System was profiled and redesigned for higher efficiency and utilising new hardware",
            "Fully automated ansible deployment to build solution to VMs with no manual config", 
            "Developed solution into series of linux services for easy running and monitoring"
        ]
    },
    "mlgui": {
        "headline": "Machine Learning GUI",
        "intro": "Graphical user interface to facilitate simple building of machine learning models on a complex World Bank economic dataset as well as storing models and results in a database.",
        "technologies": ["Python", "ReactJS", "NodeJS", "MongoDB", "AWS EC2" ],
        "explanation": [
            "This was a personal project I undertook when I discovered a fascinating dataset provided by the world bank. The World Development Index contained information for over 250 countries, 50 years and thousands of fields, in total with over 1 million data points all about economic development. With such a rich dataset I was eager to explore and investigate it to see what insights or trends I would be able to pull out from it and after a while I had built many models and gathered even more insights.",
            "I realised if I were to build an application to handle most of the common things I would want to do with the dataset I would be able to explore it much more efficiently whilst storing my results to share in future.",
            "The main challenges this project presented were that if I wanted to be able to simply select the fields, I would need to generalise the machine learning code very nicely such that it could be passed all features and other parameters easily via variables selected in the app. This also presented a design challenge for the app itself of wanting to keep it simplistic and easy to use yet have enough detailed options to select without feeling limited.",
            "The application I built, utilised a web frontend developed using ReactJS and associated libraries. The frontend comprised of multiple selection components from which the user would select the parameters to pass into the chosen machine learning model, additionally they would be able to pass hyperparameters to the model itself if they wanted to. Once a query was submitted it would be transferred to a backend written in nodejs this handled most of the communication between components. At this point the backend would query the NoSQL database which I used to store my results, for this I used MongoDB. It would be searched when a query came in and return the results if they already existed. If not then the NodeJS backend would communicate to a second web api written in python using flask, this was done as flask could easily integrate with the machine learning scripts I had written. The machine learning was all written in python using standard data science libraries, by passing all the variables into the scripts it allowed for them to be ran using the selected characteristics from the web frontend. Once the results were calculated it would be communicated back through the chain to the NodeJS backend where it would write the results to the database so that it could be quickly recalled next time. Finally, the results were returned to the frontend to display with some simple visualisations.",
            "It was a very successful project as it allowed me to achieve my set out aims, I was able to far better explore the dataset I had found whilst saving my results so that I could recall them. I also (for a time) hosted the app on the cloud so that friends and colleagues who were interested would also be able to access my app and explore this dataset with ease."
        ],
        "outcomes": [
            "Bespoke system allowing for user selection of any features across time ranges and differing countries as well as many different machine learning methods",
            "All results and models were saved to a NoSQL database allowing for faster recall in future if same parameters were entered again",
            "System was hosted entirely in the cloud on AWS"
        ]
    },
    "socialmediagraph": {
        "headline": "Social Media Community Connectedness",
        "intro": "Investigation into Reddit communities and application of several graph algorithms to determine interconnectedness.",
        "technologies": ["Python", "Apache Spark", "Graph Theory"],
        "explanation": [
            "This project was an investigation into a large social media dataset detailing information about interactions from one community to another. The dataset had information about both the source and target community as well as when it occurred and some other sentiment analysis and properties.",
            "I thought this was a very rich dataset and was interested in trying to manually implement some graph theory algorithms to measure various properties of the communities, particularly focussing on their interconnections.",
            "The largest challenge with this project was implementing graph algorithms onto large graph datasets can take a lot of computing power, especially as the ones implemented didn't scale linearly with graph size. To mitigate this I would often take smaller subsets of the data to produce smaller graphs, once I knew the algorithm was functional I would be able to apply it to larger datasets but I was still heavily constrained by computing resource.",
            "The main algorithms I intended to implement were Karger's and Girvan-Newman's algorithms as these were two I knew and would give interesting results. Karger's algorithm is an algorithm to compute a minimum cut for a connected graph, what this means is how many is the fewest number of edges in the graph that need to be removed in order to split the graph into two disjoint components. How it works is by removing an edge at random and coalesce the connected points and edges (removing self loops), this is then repeated until only 2 nodes are left and the number of remaining edges gives the required minimum cut. The second algorithm, Girvan-Newman's, is the algorithm I used to measure connectedness of communities, it measures community structure by finding edges betweenness centrality (how many shortest paths between nodes pass through this edge). How the algorithm works is, first, calculate the edge betweenness centrality for each edge in the graph. Then the edge with the highest betweenness centrality is removed, the calculation is done again, and this is repeated until there are no more edges remaining. As the algorithm runs the network splits up into different communities and the results can be shown as a dendrogram/tree.",
            "For these algorithms I was able to implement them using both python and apache spark. The results for Girvan-Newman were particularly interesting as it showed similarly to what I intuitively expected. For instance the results showed high levels of connectedness between political communities or sports communities but lower levels of connectedness between a sport community and a political community. This was reassuring since you would expect that people who like sports are more likely to engage in discussions on many sports communities and likewise for politics, whereas there wouldn't necessarily be such a strong connection between them."
        ],
        "outcomes": [
            "Implementation of many graphing algorithms including Karger's and in both python and apache spark", 
            "Investigation results successfully mirrored expected outputs indicating a high level of confidence in the implementations",
            "Allowed for deeper investigation of how some social media communities interact and build out visualisations of large graphs of connections between communities"
        ]
    },
    "cybersecurity": {
        "headline": "Cyber Security Data Science Hub",
        "intro": "A collection of projects to build, explore and utilise data science in a cyber security context. Involved many different projects in a wide scope.",
        "technologies": ["Python", "Apache Spark", "AWS"],
        "explanation": [
            "Whilst at BT I worked as part of the cyber security data science hub (DSH), this was a team established to leverage data science for new applications in cyber security. This could entail anything from data science work to improve an exist project or product, to building new solutions, to exploring novel applications of data science to see if there was any viability to the ideas. During my time in this team I worked across several projects in some capacity and will explain a short amount about them here.",
            "One project I worked on as part of the DSH was an investigation of a new software called Apache Metron. This was a highly scalable advanced security analytics framework, additionally as it was an Apache product it was open source and community driven. It was of interest to us in BT Security as if this was going to be the next big thing in the area of cyber security frameworks we would want to know about it, and so I lead a piece of work undertaking an evaluation of it.",
            "This came with all the common problems you would expect from investigating an early stage software such as bugs, crashes and installation issues. However, once most of this was remedied I was able to start looking into it's capabilities and how easy it is to use and extend and such like. ",
            "As I was feeding back information from my evaluation it was shared with other members of BT security team, particularly those who worked on a similar product that we offered. This knowledge allowed them to develop and incorporate some of the best features and designs from Metron into our offering. I also presented some of these findings to external customers as an example of the cutting edge investigations BT Security was undertaking.",
            "Another project was working with terabytes of netflow traffic data, the aim was to see if there was any feasible way to analyse this and get useful insights from it or whether the data was just too large and too noisy to get anything useful from it.",
            "My involvements in this was early on, I helped with the investigation design as well as early stage implementation. Using a collection of aws products we were able to firstly spoof large amounts of nonsense data to check if our solution would even be able to process it fast enough and then over time we were able to adapt our data generation to make accurately faked netflow data and see whether we could pull anything from it. It was found the system could handle processing the data and so finding insights would be possible.",
            "Finally, I also took part in multiple courses whilst I was part of the DSH, the most significant of these was a week long course on ethical hacking. This was a practical course offered by EC-Council which covered many areas of cyber security and hacking including topics such as footprinting, vulnerability analysis and system hacking. Another course was multiple days on Apache Spark, this big data framework is very commonly used in the security space where the datasets are often very large and so this intensive course covered everything from basics to advanced algorithm implementations."
        ],
        "outcomes": [
            "Exploration of cutting edge data security platform, lead to features being adapted and incorporated into similar BT Security products", 
            "Early stage work on terabytes of netflow data investigation allowed for a prototype to be made and thus feasibility of gathering insights was found to be possible",
            "Completed multiple cyber security focused courses on a range of topics including ethical hacking, common attack vectors, and big data such as apache spark"
        ]
    }
}